{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178a596d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'나는': 0, '너를': 1, '사랑해': 2}\n",
      "[0, 1, 2]\n",
      "tf.Tensor([0 1 2], shape=(3,), dtype=int32)\n",
      "[[ 0.03793142 -0.02086505  0.0447511   0.03991923]\n",
      " [-0.01349724 -0.03959888  0.00891063 -0.03473145]\n",
      " [-0.04243529  0.02169938  0.01712142  0.031848  ]]\n",
      "[[ 0.02655182  0.02969113  0.01650857  0.05720312]\n",
      " [ 0.03619953  0.00891391  0.00112747  0.01821991]\n",
      " [-0.0515869   0.03455075 -0.03510647 -0.00033337]]\n",
      "[[-0.06212836 -0.00620023 -0.00482773 -0.01429406]\n",
      " [ 0.02001747 -0.03241587 -0.03564681  0.03476292]\n",
      " [ 0.02779969 -0.01811085 -0.0040384  -0.05463289]]\n",
      "[[ 0.04660624  0.02435018 -0.05015815 -0.00396201]\n",
      " [-0.01280775 -0.02385032  0.03465305  0.02537987]\n",
      " [-0.0175597  -0.02598552 -0.00515994  0.00701259]]\n"
     ]
    }
   ],
   "source": [
    "# Attention 연산을 구현해 보아요!\n",
    "import tensorflow as tf\n",
    "\n",
    "# 예제 Text\n",
    "tokens = ['나는', '너를', '사랑해']\n",
    "\n",
    "# 숫자로 변경해야 해요 (원래는 vocabulary를 이용해서 해야 해요!)\n",
    "word_to_index = {word: i for i, word in enumerate(tokens)}\n",
    "print(word_to_index) # {'나는': 0, '너를': 1, '사랑해': 2}\n",
    "\n",
    "# 숫자의 시퀀스로 변경해야해요! => [0 1 2]\n",
    "input_ids = [word_to_index[word] for word in tokens]\n",
    "print(input_ids) # [0, 1, 2]\n",
    "\n",
    "# 입력 데이터를 숫자로 표현했어요. 이제 Embedding부터 처리해 보아요!\n",
    "# Embedding 처리는 Tensorflow Keras의 Embedding Layer를 이용할거에요!\n",
    "# 입력 데이터를 Tensorflow Tensor로 변경\n",
    "input_tensor = tf.constant(input_ids)\n",
    "print(input_tensor) # tf.Tensor([0 1 2], shape=(3,), dtype=int32)\n",
    "\n",
    "# Embedding Layer\n",
    "embedding_dim = 4\n",
    "embedding_layer = tf.keras.layers.Embedding(input_dim=3,\n",
    "                                            output_dim=embedding_dim)\n",
    "embeddings = embedding_layer(input_tensor)\n",
    "print(embeddings.numpy())\n",
    "# [[ 0.03793142 -0.02086505  0.0447511   0.03991923]  '나는'\n",
    "#  [-0.01349724 -0.03959888  0.00891063 -0.03473145]  '너를'\n",
    "#  [-0.04243529  0.02169938  0.01712142  0.031848  ]] '사랑해'\n",
    "\n",
    "# Query, Key, Value를 추출하기 위해서 Dense Layer 3개가 필요!\n",
    "d_model = 4\n",
    "W_Q = tf.keras.layers.Dense(units=d_model,\n",
    "                            use_bias=False)\n",
    "W_K = tf.keras.layers.Dense(units=d_model,\n",
    "                            use_bias=False)\n",
    "W_V = tf.keras.layers.Dense(units=d_model,\n",
    "                            use_bias=False)\n",
    "\n",
    "Q = W_Q(embeddings)\n",
    "K = W_K(embeddings)\n",
    "V = W_V(embeddings)\n",
    "\n",
    "# 생성된 Query 값\n",
    "print(Q.numpy())\n",
    "# [[ 0.02655182  0.02969113  0.01650857  0.05720312]\n",
    "#  [ 0.03619953  0.00891391  0.00112747  0.01821991]\n",
    "#  [-0.0515869   0.03455075 -0.03510647 -0.00033337]]\n",
    "print(K.numpy())\n",
    "# [[-0.06212836 -0.00620023 -0.00482773 -0.01429406]\n",
    "#  [ 0.02001747 -0.03241587 -0.03564681  0.03476292]\n",
    "#  [ 0.02779969 -0.01811085 -0.0040384  -0.05463289]]\n",
    "print(V.numpy())\n",
    "# [[-0.06212836 -0.00620023 -0.00482773 -0.01429406]\n",
    "#  [ 0.02001747 -0.03241587 -0.03564681  0.03476292]\n",
    "#  [ 0.02779969 -0.01811085 -0.0040384  -0.05463289]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf70b22d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.0, shape=(), dtype=float32)\n",
      "[[-1.3655381e-03  4.8455293e-04 -1.4957197e-03]\n",
      " [-1.2850827e-03  5.1442871e-04 -7.7531178e-05]\n",
      " [ 1.5825183e-03 -4.5639355e-04 -9.4992819e-04]]\n",
      "[[0.33314216 0.33375907 0.3330988 ]\n",
      " [0.3329993  0.33359906 0.33340165]\n",
      " [0.33384144 0.33316144 0.33299708]]\n",
      "[[ 0.00540269 -0.00850394 -0.00686279  0.00948674]\n",
      " [ 0.00539276 -0.00851147 -0.00686273  0.00948536]\n",
      " [ 0.00544472 -0.00847001 -0.00691805  0.00946808]]\n"
     ]
    }
   ],
   "source": [
    "# 여기까지 수행되었으면 우리 문자열 ('나는 너를 사랑해')에 대해\n",
    "# Query, Key, Value를 구해낼 수 있어요!\n",
    "# Attention Score를 구할 수 있어요!\n",
    "d_k = tf.cast(tf.shape(K)[-1], tf.float32)\n",
    "print(d_k) # tf.Tensor(4.0, shape=(), dtype=float32)\n",
    "\n",
    "scores = tf.matmul(Q, K, transpose_b=True) / tf.math.sqrt(d_k)\n",
    "print(scores.numpy())\n",
    "# [[-1.3655381e-03  4.8455293e-04 -1.4957197e-03]\n",
    "#  [-1.2850827e-03  5.1442871e-04 -7.7531178e-05]\n",
    "#  [ 1.5825183e-03 -4.5639355e-04 -9.4992819e-04]]\n",
    "\n",
    "# 확률값으로 가중치를 구할 수 있어요!\n",
    "attention_weight = tf.nn.softmax(scores, axis=-1)\n",
    "print(attention_weight.numpy())\n",
    "# [[0.33314216 0.33375907 0.3330988 ]\n",
    "#  [0.3329993  0.33359906 0.33340165]\n",
    "#  [0.33384144 0.33316144 0.33299708]]\n",
    "\n",
    "output = tf.matmul(attention_weight, V)\n",
    "print(output.numpy())\n",
    "# [[ 0.00540269 -0.00850394 -0.00686279  0.00948674]\n",
    "#  [ 0.00539276 -0.00851147 -0.00686273  0.00948536]\n",
    "#  [ 0.00544472 -0.00847001 -0.00691805  0.00946808]]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
