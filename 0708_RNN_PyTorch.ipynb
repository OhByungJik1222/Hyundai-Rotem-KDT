{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a6d2473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMDB Dataset\n",
    "# IMDB.com 사이트에 있는 영화, 드라마 리뷰(영어)를\n",
    "# 우리가 모델로 학습할 수 있도옥 이미 단어사전도 만들고, 숫자로 다 바꿔서\n",
    "# 우리한테 제공하는 데이터셋\n",
    "# Tensorflow를 이용해서 해당 내용을 구현해 보았어요!\n",
    "# PyTorch로 구현해 보아요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e6454eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%reset -f\n",
    "\n",
    "import numpy as np\n",
    "import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from keras.datasets import imdb\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "615eedb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "# PyTorch는 직접 데이터와 모델을 GPU Memory에 이동시켜야 해요!\n",
    "# 항상 처음에 PyTorch 환경이 GPU를 사용할 수 있는 환경인지 확인!\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9598c034",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 로딩\n",
    "# 우리가 사용하는 데이터는 순차데이터(시게열데이터)\n",
    "# timestep이라는 개념이 들어가요!\n",
    "# 자연어를 처리하려고 해요! 각각의 단어( = token = timestep)\n",
    "# 이런 단어(token, timestep)의 연속이 우리의 sample이 되요!\n",
    "# 원래는 vocabulary(단어사전)이라는 것부터 만들어야 해요!\n",
    "# 그런데 IMDB Dataset은 이 과정이 이미 진행되어 있어요!\n",
    "(x_data_train, y_data_train), (x_data_test, y_data_test) = \\\n",
    "imdb.load_data(num_words=500)\n",
    "\n",
    "x_data_train, x_data_val, y_data_train, y_data_val = \\\n",
    "train_test_split(x_data_train,\n",
    "                 y_data_train,\n",
    "                 test_size=0.2,\n",
    "                 stratify=y_data_train,\n",
    "                 random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "99ab4da6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 전처리\n",
    "# 1. 길이를 맞춰야 해요!(리뷰길이가 제각각인데 동일 길이로 맞춰야 해요!)\n",
    "# Padding 처리를 통해서 길이를 맞춰요!\n",
    "x_data_train_seq = pad_sequences(x_data_train,\n",
    "                                 maxlen=100)\n",
    "x_data_val_seq = pad_sequences(x_data_val,\n",
    "                               maxlen=100)\n",
    "\n",
    "# 2. one-hot encoding으로 처리\n",
    "# token값들은 값의 볼륨이 아니에요. 분류값이에요!\n",
    "x_data_train_onehot = to_categorical(x_data_train_seq,\n",
    "                                     num_classes=500)\n",
    "x_data_val_onehot = to_categorical(x_data_val_seq,\n",
    "                                   num_classes=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b8f3a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch에서 사용하는 Tensor 형태로 변환\n",
    "# GPU를 사용하기 위해서 데이터를 이동\n",
    "x_train_tensor = torch.FloatTensor(x_data_train_onehot).to(device)\n",
    "y_train_tensor = torch.FloatTensor(y_data_train).to(device)\n",
    "\n",
    "x_val_tensor = torch.FloatTensor(x_data_val_onehot).to(device)\n",
    "y_val_tensor = torch.FloatTensor(y_data_val).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0581edb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이렇게 만든 데이터를 조금 편하게 사용해 볼거에요!\n",
    "# batch 처리하기 편하도록 DataLoader를 이용할거에요!\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "val_dataset = TensorDataset(x_val_tensor, y_val_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=64,\n",
    "                          shuffle=True)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4a9a73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 처리가 끝났으니 Model을 구현해보면 되요!\n",
    "# 1. class를 정의해야 해요! -> 특정 class를 상속해서 만들어야 해요!\n",
    "# 2. class내에 우리가 사용할 layer를 속성으로 정의해야 해요!\n",
    "#    => __init__() 안에서 구현\n",
    "# 3. 순전파 기능을 함수로 구현해야 해요!\n",
    "#    => forward() 안에서 구현\n",
    "\n",
    "# nn.Module를 상속해서 class를 생성\n",
    "class SimpleRNNModel(nn.Module): \n",
    "    # input_size : RNN Model의 input 차원 => vocabulary의 크기\n",
    "    # hidde_size : RNN Layer안의 neuron(node) 수\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__() # 상위 class의 초기화 함수 호출\n",
    "        # PyTorch가 제공하는 RNN Layer\n",
    "        # batch_first=True는 무슨 의미인가요?\n",
    "        # 기본적인 형태는 (seq_len, batch_size, input_size) 인데\n",
    "        # (batch_size, seq_len, input_size) 이 형태가 필요해요!\n",
    "        self.rnn = nn.RNN(input_size=input_size,\n",
    "                          hidden_size=hidden_size,\n",
    "                          batch_first=True)\n",
    "        # hidden_size : 입력데이터 개수\n",
    "        # 1 : 출력데이터 개수\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    \n",
    "    # 데이터를 어떻게 전달해서 순전파 연산을 수행할건지 기술\n",
    "    def forward(self, x):\n",
    "        # out : 모든 시점의 hidden state\n",
    "        out, _ = self.rnn(x)\n",
    "        out = out[:, -1, :] # 마지막 시점의 hidden state\n",
    "        out = self.fc(out)\n",
    "        return torch.sigmoid(out)\n",
    "    \n",
    "# 이렇게 기능이 정의된 모델 클래스로부터\n",
    "# 실제로 사용할 수 있는 모델을 만들어요!\n",
    "model = SimpleRNNModel(input_size=500,\n",
    "                       hidden_size=8).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c9b05f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss, Optimizer가 있어야 해요!\n",
    "criterion = nn.BCELoss() # 'binary_crossentropy'\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=1e-3)\n",
    "\n",
    "# Tensorboard\n",
    "log_dir = './logs/' + datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
    "writer = SummaryWriter(log_dir=log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa10828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습\n",
    "# fit() 함수를 사용하지 않아요!\n",
    "# 작접 Loop를 돌면서 수동으로 처리!\n",
    "# 참고로 Lighting은 fit()을 이용해서 학습을 진행!\n",
    "\n",
    "# 20 epochs를 수행할거에요!\n",
    "epochs = 20\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # 학습\n",
    "    model.train() # PyTorch에서 모델 학습하기 전에는 반드시 이 함수를 호출\n",
    "                  # 이 작업을 수행해야 나중에 역전파를 진행할 수 없어요!\n",
    "    train_loss = 0.0\n",
    "    train_acc = 0.0\n",
    "\n",
    "    for x_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x_batch).squeeze()\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward() # backpropagation\n",
    "        optimizer.step() # weight 값 갱신\n",
    "\n",
    "        train_loss += loss.item() * 64\n",
    "        train_acc += ((outputs > 0.5).float() == y_batch).sum().item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset) # 전체 데이터 개수로 나눠줘요!\n",
    "    train_acc /= len(train_loader.dataset) # 정확도가 나와요!\n",
    "\n",
    "    # 검증\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    val_acc = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x_batch, y_batch in val_loader:\n",
    "            outputs = model(x_batch).squeeze()\n",
    "            loss = criterion(outputs, y_batch)\n",
    "\n",
    "            val_loss += loss.item() * 64\n",
    "            val_acc += ((outputs > 0.5).float() == y_batch).sum().item()\n",
    "        \n",
    "        val_loss /= len(val_loader.dataset)\n",
    "        val_acc /= len(val_loader.dataset)\n",
    "\n",
    "    print(f'Epochs : {epoch}/{epochs}') \n",
    "    print(f'Train Accuracy : {train_acc} Train Loss : {train_loss}')\n",
    "    print(f'Validation Accuracy : {val_acc} Validation Loss : {val_loss}')\n",
    "# Epochs : 20/20\n",
    "# Train Accuracy : 0.8127 Train Loss : 0.4260636926651001\n",
    "# Validation Accuracy : 0.7652 Validation Loss : 0.5257220012664795"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
